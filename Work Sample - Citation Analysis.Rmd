---
title: "Arlington County Traffic Summons and Parking Citation Analysis"
author: "Matthew Harris"
date: 11/25/2018
output:
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Introduction
This will be used as a sample to display some of my analytical capabilities in R. This project will focus on demonstrating how useful R can be to perform analysis that is easy to reproduce and communicate. This is by no means an exhaustive demonstration of my proficiency with R, but should highlight common data analysis functions that I perform regularly.

##Analysis Goals
Parking citations are never fun, but analyzing them could be. In this exploratory analysis I will examine the data to determine what types of citations are recorded the most, and if there are any discernable patterns over different timeframes. I will refer to both parking citations and traffic summons as citations for the remainder of the analysis. It is important to note that since this data was only collected on vehicles that received citations I have no means of accurately predicting the probability of future outcomes.

This project will also include a Shiny application that will allow users to filter and segment the data without having to interact with the underlying code. Feel free to play with the application first and return to this document to see my process of cleaning and analyzing the data. [Shiny Application]()

Lets dig in :)

##Data Sources
The traffic summons and parking citations data that I will be using are for Arlington County, Virginia. This data can be found on the Arlington County [website](https://data.arlingtonva.us/datasets/183803/traffic-summons-and-parking-citations/).

##Data Import
Loading necessary packages for analysis.
```{r, message = FALSE}
library(lubridate)
library(scales)
library(GGally)
library(tidyverse)
```

Loading csv file containing the data.
```{r, message = FALSE}
citation_data <- read_csv("Raw Data/Traffic Summons and Parking Citations.csv")
```

##Data Wrangling/Cleansing
###Data Inspection
In order to begin the wrangling/cleansing process I need to know the characteristics of the data. 

```{r}
summary(citation_data)
```
###Data Type Updates
The first thing that stands out is that the `TicketDateTime` variable is classified as `character`. I will need to change that variable to be classified as `POSIXct` in order to determine when certain events occured. I will also change all variables names to lower case.

```{r}
citation_data %>%
  mutate_at(1, mdy_hms) -> citation_data

citation_data %>%
  colnames() %>%
  tolower() -> colnames(citation_data)
```

###Missing Data
Since all of the other variables are classified as `character` they don't display any NA information in the `summary` function. I can summarize the data to determine if NAs are present and how many.

```{r}
str(citation_data)

citation_data %>%
  select(everything()) %>%
  summarize_all(funs(sum(is.na(.))))
```
There appears to be a lot of missing data for the `address` and `location` variables. The dataset is still viable for analysis as the other variables don't caontain NAs. I can also see from the `str` function that the `address` variable contains street addresses. The first couple of `location` oberservations are NA so I will need to use a differemt summary operation to see what this variable holds.
```{r}
citation_data %>%
  filter(!is.na(location)) %>%
  select(location)

citation_data %>%
  select(source) %>%
  distinct()
```
The `location` variable appears to hold longitude and latitude data for the location of the citation. This data would be interesting to use but it looks as though some observations only have one location parameter. There are other methods for determining geographical location using street addresses that can be used. The `source` variable identifies whether the observation is for a parking citation or traffic summon. This could be useful for filtering later in the analysis. <br />

###Data Characteristics
Now that all of the variables have been examined I can look at the `ticketdatetime` variable again to determine the date range of the data. 

```{r}
summary(citation_data)
```
The obeservations appears to have occured over alomst an entire calendar year. Hopefully this will allow me to determine if there were any seasonal patterns that occured in that timeframe. 

##Data Transformation
My next step is to create new variables to examine the data at different timeframes other than an exact date and time.
```{r}
citation_data %>%
  mutate(ticket_hour = hour(ticketdatetime), ticket_day = day(ticketdatetime),
         ticket_weekday = weekdays(ticketdatetime),
         ticket_month = month(ticketdatetime)) -> citation_data
```
##Analysis
Now that the data has been cleaned and transformed I can begin to answer some of my initial questions. First up what type of citation is most common from the data?

###Top Citations
The first block of code below tells me that there are 254 different citation types recorded. With that many unique types I decided to filter the data for the top 7 in term of number of citations recorded. Expired meter citations make up nearly 32% of the whole dataset with safety inspection violations coming in at second with a little over 7%. It's clear that expired meter citations are the bread and butter of Arlington County traffic police. Further analysis can be done to determine why these are so much more common.
```{r}
citation_data %>%
  select(ticketchargedescription) %>%
  distinct() %>%
  count()

citation_data %>%
  group_by(ticketchargedescription) %>%
  summarize(ticket_type_count = n()) %>%
  ungroup() %>%
  mutate(ticket_type_perc = ticket_type_count/sum(ticket_type_count)) %>%
  arrange(desc(ticket_type_count)) %>%
  head(7) -> top_ticket_types
top_ticket_types

top_ticket_types %>%
  ggplot(aes(reorder(ticketchargedescription, -ticket_type_perc), ticket_type_perc, fill = factor(ticketchargedescription))) + 
  geom_col() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5), legend.position = "none") + 
  labs(x = "Citation Type", y = "% of Total") + scale_y_continuous(labels = percent)
```

###Citation Distribution Over Time
Now that we know what the top citation types are lets see when they occur throughout a day. I decided to visualize this information with a point/line plot, but a similar output could have been created with a histogram.
```{r}
top_ticket_types %>% pull(ticketchargedescription) -> top_ticket_vector

citation_data %>%
  filter(ticketchargedescription %in% top_ticket_vector) %>%
  group_by(ticket_hour, ticketchargedescription) %>%
  summarize(ticket_hour_count = n()) %>%
  ungroup() %>%
  ggplot(aes(ticket_hour, ticket_hour_count, col = factor(ticketchargedescription))) + geom_line() + 
  geom_point() + scale_x_continuous(breaks = seq.int(0, 24, 2)) + labs(x = "Hour", y = "Citation Count") +
  scale_color_discrete(name = "Citation Description")
```

A quick glance at the graph shows us that citations seem to spike around 10:00 AM and 2:00 PM for nearly all of the types shown. It's also clear that there are sharp drop offs early in the morning and late at night. The RPP(Residential Permit Parking) citations seem to spike at 9:00 PM which make sense considering that is when most residential areas begin looking for parking violators.

###Citation Locations - Missing Data
Lets now examine where the citations were given the most. We learned ealrier that there are 23,536 observations that don't include `address` data. We can examine this data first to determine a reason for these missing data points.

```{r}
citation_data %>%
  filter(is.na(address)) %>%
  group_by(ticketchargedescription, address) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(perc_na = n/sum(n)) %>%
  arrange(desc(n)) -> address_na
address_na
```
A quick summary function shows us that `r address_na %>% pull(perc_na) %>% head(10) %>% sum() %>% round(2) %>% percent()` of the NA location observations are related to speeding and traffic related citations. It's likely that the officers giving out the citations didn't have time to record where they occured. Either way we now know that none of the top citation types are missing `address` data.

###Citation Locations - Most Common
Running the opposite of the previous summary function shows us the top locations where citations were received.
```{r}
citation_data %>%
  filter(!is.na(address)) %>%
  group_by(address) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(perc_na = n/sum(n)) %>%
  arrange(desc(n))
```

1400 COURTHOUSE RD N is the address with the most citations. A quick Google Maps search lets us know that a large parking lot is located at that address. I could also use the `ggmap` package to automate this address lookup process, but recent changes to Google's Maps Static API have made that difficult.

![](top_address.png)

##Alternate Visualization Methods
So far I have created tables and visualizations that are based on particular criteria. There are numerous ways to slice this data set within R but having dynamic plots would make my exploratory analysis easier. I can share my findings by creating a Shiny application that anyone can be utilized by anyone, regardless of their experience with R.<br /> [Shiny Application]()

```{r, eval = FALSE}
rm(address_na)
save.image(file = "Processed Data/citation_data_tidy.RData")
```